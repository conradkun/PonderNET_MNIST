{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PonderNet_MNIST.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "248001e633b18adad6ee4bf0c40de61885f89dde724395bcd76c98f49c87f963"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('semproj': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkNC-cZfUijV"
      },
      "source": [
        "This code is adopted from one of the [PyTorch Lightning examples](https://colab.research.google.com/drive/1Tr9dYlwBKk6-LgLKGO8KYZULnguVA992?usp=sharing#scrollTo=CxXtBfFrKYgA) and [this PonderNet implementation](https://nn.labml.ai/adaptive_computation/ponder_net/index.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jen00bM0RCho"
      },
      "source": [
        "# PonderNet: complexity in MNIST\n",
        "[PonderNet](https://arxiv.org/pdf/2107.05407.pdf) is a new architecture that promises to be able to adapt its computational budget according to the complexity of the task at hand. In the original paper, the authors deal with either problems that are too simplistic (guessing the parity of the number of \"1\"s in a vector), or too obscure to be able to draw meaningful conclusions. Although their results show that harder problems are given more computational resources than easier ones by PonderNet, it is hard to say if this is only the case in the selected toy examples.\n",
        "\n",
        "In order to test the validity of their argument, we train a version of PonderNet on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). Our solution uses an augmented CNN network that embeds the image so it can be adapted to the PonderNet framework, which requires the incorporation of a hidden state.\n",
        "\n",
        "We propose two tasks, an _interpolation_ and an _extrapolation_ one, which are akin to the ones found in the parity experiments from the original paper. Our interpolation task consists on simply learning to correctly classify the unaltered MNIST dataset. On the other hand, our extrapolation task consists on training PonderNet on slightly rotated images and testing it on considerably rotated images; this mirrors how the parity extrapolation task is trained on vectors of size 1-48 and tested on sizes between 48-96.\n",
        "\n",
        "Our results show that PonderNet is able to solve the interpolation task but struggles with the extrapolation task. This is expected since pronounced rotations can render an image impossible to classify. In terms of the expected number of steps, the extrapolation task uses less steps the more complex it is; this is a counter-intuitive result for which it is hard to draw meaningful conclusions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVeQy_nxGrr_"
      },
      "source": [
        "# Setup and imports\n",
        "\n",
        "We use `PyTorch Lightning` (wrapping `PyTorch`) as our main framework and `wandb` to track and log the experiments. We set all seeds through `PyTorch Lightning`'s dedicated function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6RExf7OY4GN"
      },
      "source": [
        "!pip install wandb\n",
        "!pip install pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPOQFIOpUijb"
      },
      "source": [
        "# import Libraries\n",
        "\n",
        "# torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "\n",
        "# pl imports\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
        "\n",
        "# remaining imports\n",
        "import wandb\n",
        "from math import floor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D383rjfDo2qS"
      },
      "source": [
        "# set seeds\n",
        "seed_everything(1234)\n",
        "\n",
        "# log in to wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsDmEtGxHCMo"
      },
      "source": [
        "# Constants and hyeperparameters\n",
        "\n",
        "We define the hyperparameters for our experiments. The choices for the underlying CNN are taken from the linked MNIST tutorial, and similarly with the PonderNet hyperparameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrFRWUHEUijd"
      },
      "source": [
        "# TRAINER SETTINGS\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "\n",
        "# OPTIMIZER SETTINGS\n",
        "LR = 0.001\n",
        "GRAD_NORM_CLIP = 0.5\n",
        "\n",
        "# MODEL HPARAMS\n",
        "N_HIDDEN = 64\n",
        "N_HIDDEN_CNN = 64\n",
        "N_HIDDEN_LIN = 64\n",
        "KERNEL_SIZE = 5\n",
        "\n",
        "MAX_STEPS = 20\n",
        "LAMBDA_P = 0.2\n",
        "BETA = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6MXcE-76R3g"
      },
      "source": [
        "# MNIST\n",
        "\n",
        "We wrap the MNIST dataset with `PyTorch Lightning`'s Data Module classs, which allows for easier integration. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMWotv1S3M9V"
      },
      "source": [
        "class MNIST_DataModule(pl.LightningDataModule):\n",
        "    '''\n",
        "        DataModule to hold the MNIST dataset. Accepts different transforms for train and test to\n",
        "        allow for extrapolation experiments.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data_dir : str\n",
        "            Directory where MNIST will be downloaded or taken from.\n",
        "\n",
        "        train_transform : [transform] \n",
        "            List of transformations for the training dataset. The same\n",
        "            transformations are also applied to the validation dataset.\n",
        "\n",
        "        test_transform : [transform] or [[transform]]\n",
        "            List of transformations for the test dataset. Also accepts a list of\n",
        "            lists to validate on multiple datasets with different transforms.\n",
        "\n",
        "        batch_size : int\n",
        "            Batch size for both all dataloaders.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, data_dir='./', train_transform=None, test_transform=None, batch_size=256):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.train_transform = train_transform\n",
        "        self.test_transform = test_transform\n",
        "\n",
        "        self.default_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        '''called only once and on 1 GPU'''\n",
        "        # download data (train/val and test sets)\n",
        "        MNIST(self.data_dir, train=True, download=True)\n",
        "        MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        '''\n",
        "            Called on each GPU separately - stage defines if we are\n",
        "            at fit, validate, test or predict step.\n",
        "        '''\n",
        "        # we set up only relevant datasets when stage is specified\n",
        "        if stage in [None, 'fit', 'validate']:\n",
        "            mnist_train = MNIST(self.data_dir, train=True,\n",
        "                                transform=(self.train_transform or self.default_transform))\n",
        "            self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "        if stage == 'test' or stage is None:\n",
        "            if self.test_transform is None or isinstance(self.test_transform, transforms.Compose):\n",
        "                self.mnist_test = MNIST(self.data_dir,\n",
        "                                        train=False,\n",
        "                                        transform=(self.test_transform or self.default_transform))\n",
        "            else:\n",
        "                self.mnist_test = [MNIST(self.data_dir,\n",
        "                                         train=False,\n",
        "                                         transform=test_transform)\n",
        "                                   for test_transform in self.test_transform]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        '''returns training dataloader'''\n",
        "        mnist_train = DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n",
        "        return mnist_train\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        '''returns validation dataloader'''\n",
        "        mnist_val = DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
        "        return mnist_val\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        '''returns test dataloader(s)'''\n",
        "        if isinstance(self.mnist_test, MNIST):\n",
        "            return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
        "\n",
        "        mnist_test = [DataLoader(test_dataset,\n",
        "                                 batch_size=self.batch_size)\n",
        "                      for test_dataset in self.mnist_test]\n",
        "        return mnist_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYh4cuLNMo3M"
      },
      "source": [
        "# PonderNet implementation\n",
        "\n",
        "## Auxiliary networks\n",
        "\n",
        "The following section contains code that implements our particular version of PonderNet. For convenience, we define two small networks that will be used within PonderNet, a CNN and a multi-layer perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s-YObeD1kBY"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    '''\n",
        "        Simple 3-layer multi layer perceptron.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_input : int\n",
        "            Size of the input.\n",
        "\n",
        "        n_hidden : int\n",
        "            Number of units of the hidden layer.\n",
        "\n",
        "        n_ouptut : int\n",
        "            Size of the output.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_input, n_hidden, n_output):\n",
        "        super(MLP, self).__init__()\n",
        "        self.i2h = nn.Linear(n_input, n_hidden)\n",
        "        self.h2o = nn.Linear(n_hidden, n_output)\n",
        "        self.droput = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''forward pass'''\n",
        "        x = F.relu(self.i2h(x))\n",
        "        x = self.droput(x)\n",
        "        x = F.relu(self.h2o(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    '''\n",
        "        Simple convolutional neural network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_input : int\n",
        "            Size of the input image. We assume the image is a square,\n",
        "            and `n_input` is the size of one side.\n",
        "\n",
        "        n_ouptut : int\n",
        "            Size of the output.\n",
        "\n",
        "        kernel_size : int\n",
        "            Size of the kernel.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_input=28, n_output=50, kernel_size=5):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=kernel_size)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=kernel_size)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "\n",
        "        # calculate size of convolution output\n",
        "        self.lin_size = floor((floor((n_input - (kernel_size - 1)) / 2) - (kernel_size - 1)) / 2)\n",
        "        self.fc1 = nn.Linear(self.lin_size ** 2 * 20, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''forward pass'''\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj6tAmrohj1N"
      },
      "source": [
        "## Loss\n",
        "\n",
        "Here we define the two terms in the loss, namely the reconstruction term and the regularization term. We create a class to wrap them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhVDCe711fQG"
      },
      "source": [
        "class ReconstructionLoss(nn.Module):\n",
        "    '''\n",
        "        Computes the weighted average of the given loss across steps according to\n",
        "        the probability of stopping at each step.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        loss_func : callable\n",
        "            Loss function accepting true and predicted labels. It should output\n",
        "            a loss item for each element in the input batch.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, loss_func: nn.Module):\n",
        "        super().__init__()\n",
        "        self.loss_func = loss_func\n",
        "\n",
        "    def forward(self, p: torch.Tensor, y_pred: torch.Tensor, y: torch.Tensor):\n",
        "        '''\n",
        "            Compute the loss.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            p : torch.Tensor\n",
        "                Probability of halting at each step, of shape `(max_steps, batch_size)`.\n",
        "\n",
        "            y_pred : torch.Tensor\n",
        "                Predicted outputs, of shape `(max_steps, batch_size)`.\n",
        "\n",
        "            y : torch.Tensor\n",
        "                True targets, of shape `(batch_size)`.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            total_loss : torch.Tensor\n",
        "                Scalar representing the reconstruction loss.\n",
        "        '''\n",
        "        total_loss = p.new_tensor(0.)\n",
        "\n",
        "        for n in range(p.shape[0]):\n",
        "            loss = (p[n] * self.loss_func(y_pred[n], y)).mean()\n",
        "            total_loss = total_loss + loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "class RegularizationLoss(nn.Module):\n",
        "    '''\n",
        "        Computes the KL-divergence between the halting distribution generated\n",
        "        by the network and a geometric distribution with parameter `lambda_p`.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        lambda_p : float\n",
        "            Parameter determining our prior geometric distribution.\n",
        "\n",
        "        max_steps : int\n",
        "            Maximum number of allowed pondering steps.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, lambda_p: float, max_steps: int = 1_000, device=None):\n",
        "        super().__init__()\n",
        "\n",
        "        p_g = torch.zeros((max_steps,), device=device)\n",
        "        not_halted = 1.\n",
        "\n",
        "        for k in range(max_steps):\n",
        "            p_g[k] = not_halted * lambda_p\n",
        "            not_halted = not_halted * (1 - lambda_p)\n",
        "\n",
        "        self.p_g = nn.Parameter(p_g, requires_grad=False)\n",
        "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, p: torch.Tensor):\n",
        "        '''\n",
        "            Compute the loss.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            p : torch.Tensor\n",
        "                Probability of halting at each step, representing our\n",
        "                halting distribution.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            loss : torch.Tensor\n",
        "                Scalar representing the regularization loss.\n",
        "        '''\n",
        "        p = p.transpose(0, 1)\n",
        "        p_g = self.p_g[None, :p.shape[1]].expand_as(p)\n",
        "        return self.kl_div(p.log(), p_g)\n",
        "\n",
        "\n",
        "class Loss:\n",
        "    '''\n",
        "        Class to group the losses together and calculate the total loss.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rec_loss : torch.Tensor\n",
        "            Reconstruction loss obtained from running the network.\n",
        "\n",
        "        reg_loss : torch.Tensor\n",
        "            Regularization loss obtained from running the network.\n",
        "\n",
        "        beta : float\n",
        "            Hyperparameter to calculate the total loss.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, rec_loss, reg_loss, beta):\n",
        "        self.rec_loss = rec_loss\n",
        "        self.reg_loss = reg_loss\n",
        "        self.beta = beta\n",
        "\n",
        "    def get_rec_loss(self):\n",
        "        '''returns the reconstruciton loss'''\n",
        "        return self.rec_loss\n",
        "\n",
        "    def get_reg_loss(self):\n",
        "        '''returns the regularization loss'''\n",
        "        return self.reg_loss\n",
        "\n",
        "    def get_total_loss(self):\n",
        "        '''returns the total loss'''\n",
        "        return self.rec_loss + self.beta * self.reg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mftlAY-ShyLX"
      },
      "source": [
        "## PonderNet\n",
        "\n",
        "Finally, we have PonderNet. We use a `PyTorch Lichtning` module, which allows us to control all the aspects of training, validation and testing in the same class. Of special importance is the forward pass; for the sake of simplicity, we decided to implement a hardcoded maximum number of steps approach instead of a threshold on the cumulative probability of halting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MEL-HtFUijg"
      },
      "source": [
        "class PonderMNIST(pl.LightningModule):\n",
        "    '''\n",
        "        PonderNet variant to perform image classification on MNIST. It is capable of\n",
        "        adaptively choosing the number of steps for which to process an input.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_hidden : int\n",
        "            Hidden layer size of the propagated hidden state.\n",
        "\n",
        "        n_hidden_lin :\n",
        "            Hidden layer size of the underlying MLP.\n",
        "\n",
        "        n_hidden_cnn : int\n",
        "            Hidden layer size of the output of the underlying CNN.\n",
        "\n",
        "        kernel_size : int\n",
        "            Size of the kernel of the underlying CNN.\n",
        "\n",
        "        max_steps : int\n",
        "            Maximum number of steps the network is allowed to \"ponder\" for.\n",
        "\n",
        "        lambda_p : float \n",
        "            Parameter of the geometric prior. Must be between 0 and 1.\n",
        "\n",
        "        beta : float\n",
        "            Hyperparameter to calculate the total loss.\n",
        "\n",
        "        lr : float\n",
        "            Learning rate.\n",
        "\n",
        "        Modules\n",
        "        -------\n",
        "        cnn : CNN\n",
        "            Learnable convolutional neural network to emgbed the image into a vector.\n",
        "\n",
        "        mlp : MLP\n",
        "            Learnable 3-layer machine learning perceptron to combine the hidden state with\n",
        "            the image embedding.\n",
        "\n",
        "        ouptut_layer : nn.Linear\n",
        "            Linear module that serves as a multi-class classifier.\n",
        "\n",
        "        lambda_layer : nn.Linear\n",
        "            Linear module that generates the halting probability at each step.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_hidden, n_hidden_lin, n_hidden_cnn, kernel_size, max_steps, lambda_p, beta, lr):\n",
        "        super().__init__()\n",
        "\n",
        "        # attributes\n",
        "        self.n_classes = 10\n",
        "        self.max_steps = max_steps\n",
        "        self.lambda_p = lambda_p\n",
        "        self.beta = beta\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # modules\n",
        "        self.cnn = CNN(n_input=28, kernel_size=kernel_size, n_output=n_hidden_cnn)\n",
        "        self.mlp = MLP(n_input=n_hidden_cnn + n_hidden, n_hidden=n_hidden_lin, n_output=n_hidden)\n",
        "        self.outpt_layer = nn.Linear(n_hidden, self.n_classes)\n",
        "        self.lambda_layer = nn.Linear(n_hidden, 1)\n",
        "\n",
        "        # losses\n",
        "        self.loss_rec = ReconstructionLoss(nn.CrossEntropyLoss())\n",
        "        self.loss_reg = RegularizationLoss(self.lambda_p, max_steps=self.max_steps, device=self.device)\n",
        "\n",
        "        # metrics\n",
        "        self.accuracy = torchmetrics.Accuracy()\n",
        "\n",
        "        # save hparams on W&B\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "            Run the forward pass.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            x : torch.Tensor\n",
        "                Batch of input features of shape `(batch_size, n_elems)`.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            y : torch.Tensor\n",
        "                Tensor of shape `(max_steps, batch_size)` representing\n",
        "                the predictions for each step and each sample. In case\n",
        "                `allow_halting=True` then the shape is\n",
        "                `(steps, batch_size)` where `1 <= steps <= max_steps`.\n",
        "\n",
        "            p : torch.Tensor\n",
        "                Tensor of shape `(max_steps, batch_size)` representing\n",
        "                the halting probabilities. Sums over rows (fixing a sample)\n",
        "                are 1. In case `allow_halting=True` then the shape is\n",
        "                `(steps, batch_size)` where `1 <= steps <= max_steps`.\n",
        "\n",
        "            halting_step : torch.Tensor\n",
        "                An integer for each sample in the batch that corresponds to\n",
        "                the step when it was halted. The shape is `(batch_size,)`. The\n",
        "                minimal value is 1 because we always run at least one step.\n",
        "        '''\n",
        "        # extract batch size for QoL\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # propagate to get h_1\n",
        "        h = x.new_zeros((batch_size, self.n_hidden))\n",
        "        embedding = self.cnn(x)\n",
        "        concat = torch.cat([embedding, h], 1)\n",
        "        h = self.mlp(concat)\n",
        "\n",
        "        # lists to save p_n, y_n\n",
        "        p = []\n",
        "        y = []\n",
        "\n",
        "        # vectors to save intermediate values\n",
        "        un_halted_prob = h.new_ones((batch_size,))  # unhalted probability till step n\n",
        "        halting_step = h.new_zeros((batch_size,), dtype=torch.long)  # stopping step\n",
        "\n",
        "        # main loop\n",
        "        for n in range(1, self.max_steps + 1):\n",
        "            # obtain lambda_n\n",
        "            if n == self.max_steps:\n",
        "                lambda_n = h.new_ones(batch_size)\n",
        "            else:\n",
        "                lambda_n = torch.sigmoid(self.lambda_layer(h)).squeeze()\n",
        "\n",
        "            # obtain output and p_n\n",
        "            y_n = self.outpt_layer(h)\n",
        "            p_n = un_halted_prob * lambda_n\n",
        "\n",
        "            # append p_n, y_n\n",
        "            p.append(p_n)\n",
        "            y.append(y_n)\n",
        "\n",
        "            # calculate halting step\n",
        "            halting_step = torch.maximum(\n",
        "                n\n",
        "                * (halting_step == 0)\n",
        "                * torch.bernoulli(lambda_n).to(torch.long),\n",
        "                halting_step)\n",
        "\n",
        "            # track unhalted probability and flip coin to halt\n",
        "            un_halted_prob = un_halted_prob * (1 - lambda_n)\n",
        "\n",
        "            # propagate to obtain h_n\n",
        "            embedding = self.cnn(x)\n",
        "            concat = torch.cat([embedding, h], 1)\n",
        "            h = self.mlp(concat)\n",
        "\n",
        "            # break if we are in inference and all elements have halting_step\n",
        "            if not self.training and (halting_step > 0).sum() == batch_size:\n",
        "                break\n",
        "\n",
        "        return torch.stack(y), torch.stack(p), halting_step\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        '''\n",
        "            Perform the training step.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            batch : (torch.Tensor, torch.Tensor)\n",
        "                Current training batch to train on.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            loss : torch.Tensor\n",
        "                Loss value of the current batch.\n",
        "        '''\n",
        "        loss, _, acc, steps = self._get_loss_and_metrics(batch)\n",
        "\n",
        "        # logging\n",
        "        self.log('train/steps', steps)\n",
        "        self.log('train/accuracy', acc)\n",
        "        self.log('train/total_loss', loss.get_total_loss())\n",
        "        self.log('train/reconstruction_loss', loss.get_rec_loss())\n",
        "        self.log('train/regularization_loss', loss.get_reg_loss())\n",
        "\n",
        "        return loss.get_total_loss()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        '''\n",
        "            Perform the validation step. Logs relevant metrics and returns\n",
        "            the predictions to be used in a custom callback.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            batch : (torch.Tensor, torch.Tensor)\n",
        "                Current validation batch to evaluate.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            preds : torch.Tensor\n",
        "                Predictions for the current batch.\n",
        "        '''\n",
        "        loss, preds, acc, steps = self._get_loss_and_metrics(batch)\n",
        "\n",
        "        # logging\n",
        "        self.log('val/steps', steps)\n",
        "        self.log('val/accuracy', acc)\n",
        "        self.log('val/total_loss', loss.get_total_loss())\n",
        "        self.log('val/reconstruction_loss', loss.get_rec_loss())\n",
        "        self.log('val/regularization_loss', loss.get_reg_loss())\n",
        "\n",
        "        # for custom callback\n",
        "        return preds\n",
        "\n",
        "    def test_step(self, batch, batch_idx, dataset_idx=0):\n",
        "        '''\n",
        "            Perform the test step. Returns relevant metrics.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            batch : (torch.Tensor, torch.Tensor)\n",
        "                Current teest batch to evaluate.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            acc : torch.Tensor\n",
        "                Accuracy for the current batch.\n",
        "\n",
        "            steps : torch.Tensor\n",
        "                Average number of steps for the current batch.\n",
        "        '''\n",
        "        _, _, acc, steps = self._get_loss_and_metrics(batch)\n",
        "\n",
        "        # logging\n",
        "        self.log(f'test_{dataset_idx}/steps', steps)\n",
        "        self.log(f'test_{dataset_idx}/accuracy', acc)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        '''\n",
        "            Configure the optimizers and learning rate schedulers.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            config : dict\n",
        "                Dictionary with `optimizer` and `lr_scheduler` keys, with an\n",
        "                optimizer and a learning scheduler respectively.\n",
        "        '''\n",
        "        optimizer = Adam(self.parameters(), lr=self.lr)\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": ReduceLROnPlateau(optimizer, mode='max', verbose=True),\n",
        "                \"monitor\": 'val/accuracy',\n",
        "                \"interval\": 'epoch',\n",
        "                \"frequency\": 1\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def configure_callbacks(self):\n",
        "        '''returns a list of callbacks'''\n",
        "        # we choose high patience sine we validate 4 times per epoch to have nice graphs\n",
        "        early_stopping = EarlyStopping(monitor='val/accuracy', mode='max', patience=6)\n",
        "        model_checkpoint = ModelCheckpoint(monitor=\"val/accuracy\", mode='max')\n",
        "        return [early_stopping, model_checkpoint]\n",
        "\n",
        "    def _get_loss_and_metrics(self, batch):\n",
        "        '''\n",
        "            Returns the losses, the predictions, the accuracy and the number of steps.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            batch : (torch.Tensor, torch.Tensor)\n",
        "                Batch to process.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            loss : Loss\n",
        "                Loss object from which all three losses can be retrieved.\n",
        "\n",
        "            preds : torch.Tensor\n",
        "                Predictions for the current batch.\n",
        "\n",
        "            acc : torch.Tensor\n",
        "                Accuracy obtained with the current batch.\n",
        "\n",
        "            steps : torch.Tensor\n",
        "                Average number of steps in the current batch.\n",
        "        '''\n",
        "        # extract the batch\n",
        "        data, target = batch\n",
        "\n",
        "        # forward pass\n",
        "        y, p, halted_step = self(data)\n",
        "\n",
        "        # remove elements with infinities (after taking the log)\n",
        "        if torch.any(p == 0) and self.training:\n",
        "            valid_indices = torch.all(p != 0, dim=0)\n",
        "            p = p[:, valid_indices]\n",
        "            y = y[:, valid_indices]\n",
        "            halted_step = halted_step[valid_indices]\n",
        "            target = target[valid_indices]\n",
        "\n",
        "        # calculate the loss\n",
        "        loss_rec_ = self.loss_rec(p, y, target)\n",
        "        loss_reg_ = self.loss_reg(p)\n",
        "        loss = Loss(loss_rec_, loss_reg_, self.beta)\n",
        "\n",
        "        halted_index = (halted_step - 1).unsqueeze(0).unsqueeze(2).repeat(1, 1, self.n_classes)\n",
        "\n",
        "        # calculate the accuracy\n",
        "        logits = y.gather(dim=0, index=halted_index).squeeze()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = self.accuracy(preds, target)\n",
        "\n",
        "        # calculate the average number of steps\n",
        "        steps = (halted_step * 1.0).mean()\n",
        "\n",
        "        return loss, preds, acc, steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G24_goMvOgd5"
      },
      "source": [
        "# Run interpolation\n",
        "\n",
        "Load the MNIST dataset with no rotations and train PonderNet on it. Make sure to edit the `WandbLogger` call so that you log the experiment on your account's desired project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUC__pOz6y9g"
      },
      "source": [
        "# initialize datamodule and model\n",
        "mnist = MNIST_DataModule(batch_size=BATCH_SIZE)\n",
        "model = PonderMNIST(n_hidden=N_HIDDEN,\n",
        "                    n_hidden_cnn=N_HIDDEN_CNN,\n",
        "                    n_hidden_lin=N_HIDDEN_LIN,\n",
        "                    kernel_size=KERNEL_SIZE,\n",
        "                    max_steps=MAX_STEPS,\n",
        "                    lambda_p=LAMBDA_P,\n",
        "                    beta=BETA,\n",
        "                    lr=LR)\n",
        "\n",
        "# setup logger\n",
        "logger = WandbLogger(project='PonderNet', name='interpolation', offline=False)\n",
        "logger.watch(model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    logger=logger,                      # W&B integration\n",
        "    gpus=-1,                            # use all available GPU's\n",
        "    max_epochs=EPOCHS,                  # maximum number of epochs\n",
        "    gradient_clip_val=GRAD_NORM_CLIP,   # gradient clipping\n",
        "    val_check_interval=0.25,            # validate 4 times per epoch\n",
        "    precision=16,                       # train in half precision\n",
        "    deterministic=True)                 # for reproducibility\n",
        "\n",
        "# fit the model\n",
        "trainer.fit(model, datamodule=mnist)\n",
        "\n",
        "# evaluate on the test set\n",
        "trainer.test(model, datamodule=mnist)\n",
        "\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1c2moISO-Uv"
      },
      "source": [
        "# Run extrapolation\n",
        "Train PonderNet on slightly rotated MNIST pictures, while testing on more pronounced rotations. As before, make sure to edit the `WandbLogger` accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtpNqmooYpv4"
      },
      "source": [
        "def get_transforms():\n",
        "    # define transformations\n",
        "    transform_22 = transforms.Compose([\n",
        "        transforms.RandomRotation(degrees=22.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    transform_45 = transforms.Compose([\n",
        "        transforms.RandomRotation(degrees=45),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    transform_67 = transforms.Compose([\n",
        "        transforms.RandomRotation(degrees=67.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    transform_90 = transforms.Compose([\n",
        "        transforms.RandomRotation(degrees=90),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_transform = transform_22\n",
        "    test_transform = [transform_22, transform_45, transform_67, transform_90]\n",
        "\n",
        "    return train_transform, test_transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTXoZv32oAk8"
      },
      "source": [
        "train_transform, test_transform = get_transforms()\n",
        "\n",
        "# initialize datamodule and model\n",
        "mnist = MNIST_DataModule(batch_size=BATCH_SIZE,\n",
        "                         train_transform=train_transform,\n",
        "                         test_transform=test_transform)\n",
        "model = PonderMNIST(n_hidden=N_HIDDEN,\n",
        "                    n_hidden_cnn=N_HIDDEN_CNN,\n",
        "                    n_hidden_lin=N_HIDDEN_LIN,\n",
        "                    kernel_size=KERNEL_SIZE,\n",
        "                    max_steps=MAX_STEPS,\n",
        "                    lambda_p=LAMBDA_P,\n",
        "                    beta=BETA,\n",
        "                    lr=LR)\n",
        "\n",
        "# setup logger\n",
        "logger = WandbLogger(project='PonderNet', name='extrapolation', offline=False)\n",
        "logger.watch(model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    logger=logger,                      # W&B integration\n",
        "    gpus=-1,                            # use all available GPU's\n",
        "    max_epochs=EPOCHS,                  # maximum number of epochs\n",
        "    gradient_clip_val=GRAD_NORM_CLIP,   # gradient clipping\n",
        "    val_check_interval=0.25,            # validate 4 times per epoch\n",
        "    precision=16,                       # train in half precision\n",
        "    deterministic=True)                 # for reproducibility\n",
        "\n",
        "# fit the model\n",
        "trainer.fit(model, datamodule=mnist)\n",
        "\n",
        "# evaluate on the test set\n",
        "trainer.test(model, datamodule=mnist)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}