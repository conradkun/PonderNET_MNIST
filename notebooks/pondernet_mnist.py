# -*- coding: utf-8 -*-
"""PonderNet_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/187SeZIydG_V_OiA-oxYRFSNlhMKeVUar

This code is adopted from one of the [PyTorch Lightning examples](https://colab.research.google.com/drive/1Tr9dYlwBKk6-LgLKGO8KYZULnguVA992?usp=sharing#scrollTo=CxXtBfFrKYgA) and [this PonderNet implementation](https://nn.labml.ai/adaptive_computation/ponder_net/index.html).

# PonderNet & MNIST
[PonderNet](https://arxiv.org/pdf/2107.05407.pdf) is a new architecture that promises to be able to adapt its computational budget according to the complexity of the task at hand. In the original paper, the authors deal with either problems that are too simplistic (guessing the parity of the number of "1"s in a vector), or too obscure to be able to draw meaningful conclusions. Although their results show that harder problems are given more computational resources than easier ones by PonderNet, it is hard to say if this is only the case in the selected toy examples.

In order to test the validity of their argument, we train a version of PonderNet on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). Our solution uses an augmented CNN network that embeds the image so it can be adapted to the PonderNet framework, which requires the incorporation of a hidden state.

We propose two tasks, an _interpolation_ and an _extrapolation_ one, which are akin to the ones found in the parity experiments from the original paper. Our interpolation task consists on simply learning to correctly classify the unaltered MNIST dataset. On the other hand, our extrapolation tast consists on training PonderNet on slightly rotated images and testing it on considerably rotated images; this mirrors how the parity extrapolation task is trained on vectors of size 1-48 and tested on sizes between 48-96.

Our results show that PonderNet is able to solve the interpolation task (95.5% accuracy) but struggles with the extrapolation task (70% accuracy). This may be linked to the way in which our network processes the input (one column at a time), which is less robust to rotations. In terms of the expected number of steps, the extrapolation task uses slighly more steps; it is hard to say if the difference is significant, but at least it seems to correspond with the authors' claims.

# Setup and imports

We use `PyTorch Lightning` (wrapping `PyTorch`) as our main framework and `wandb` to track and log the experiments. We set all seeds through `PyTorch Lightning`'s dedicated function.
"""


# import Libraries

# torch imports
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
from torchvision.datasets import MNIST
import torch.nn.functional as F
import torchmetrics

# pl imports
import pytorch_lightning as pl
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.callbacks import Callback
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint

# remaining imports
import wandb
from math import floor

# set seeds
seed_everything(1234)

"""# Constants and hyeperparameters

We use `wandb.config` so our hyperparameters can be stored for each experiment. The choices for the underlying CNN are taken from the linked MNIST tutorial, and similarly with the PonderNet hyperparameters. 
"""

config = wandb.config

# trainer settings
config.batch_size = 128
config.epochs = 10
config.num_workers = 0

# optimizer settings
config.lr = 0.001
config.grad_norm_clip = 0.5

# model hparams
config.n_classes = 10
config.n_input = 28  # we assume square pictures
config.n_hidden = 64
config.n_hidden_cnn = 64
config.n_hidden_lin = 64
config.kernel_size = 5

config.max_steps = 20
config.lambda_p = 0.2
config.beta = 0.01

"""# MNIST

We wrap the MNIST dataset with `PyTorch Lightning`'s Data Module classs, which allows for easier integration. 

"""


class MNIST_DataModule(pl.LightningDataModule):
    def __init__(self, data_dir='./', batch_size=256, num_workers=8, train_transform=None, test_transform=None):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.num_workers = num_workers

        if train_transform is None:
            self.train_transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.1307,), (0.3081,))
            ])
        else:
            self.train_transform = train_transform

        if test_transform is None:
            self.test_transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.1307,), (0.3081,))
            ])
        else:
            self.test_transform = test_transform

    def prepare_data(self):
        '''called only once and on 1 GPU'''
        # download data (train/val and test sets)
        MNIST(self.data_dir, train=True, download=True)
        MNIST(self.data_dir, train=False, download=True)

    def setup(self, stage=None):
        '''called on each GPU separately - stage defines if we are at fit, validate, test or predict step'''
        # we set up only relevant datasets when stage is specified (automatically set by Lightning)
        if stage in [None, 'fit', 'validate']:
            mnist_train = MNIST(self.data_dir, train=True, transform=self.train_transform)
            self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])
        if stage == 'test' or stage is None:
            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.test_transform)

    def train_dataloader(self):
        mnist_train = DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)
        return mnist_train

    def val_dataloader(self):
        mnist_val = DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)
        return mnist_val

    def test_dataloader(self):
        mnist_test = DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)
        return mnist_test


"""# PonderNet implementation

## Auxiliary networks

The following section contains code that implements our particular version of PonderNet. For convenience, we define two small networks that will be used within PonderNet, a CNN and a multi-layer perceptron.
"""


class CNN(nn.Module):
    def __init__(self, n_input=28, n_hidden=50, kernel_size=5):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=kernel_size)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=kernel_size)
        self.conv2_drop = nn.Dropout2d()

        # calculate size of convolution output
        self.lin_size = floor((floor((n_input - (kernel_size - 1)) / 2) - (kernel_size - 1)) / 2)
        self.fc1 = nn.Linear(self.lin_size ** 2 * 20, n_hidden)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = torch.flatten(x, 1)
        if(x.size()[1] != self.lin_size):
            print(self.lin_size)
            print(x.size()[1])
        x = F.relu(self.fc1(x))
        return x


class MLP(nn.Module):
    def __init__(self, n_input, n_hidden, n_output):
        super(MLP, self).__init__()
        self.i2h = nn.Linear(n_input, n_hidden)
        self.h2o = nn.Linear(n_hidden, n_output)
        self.droput = nn.Dropout(0.2)

    def forward(self, x):
        x = F.relu(self.i2h(x))
        x = self.droput(x)
        x = F.relu(self.h2o(x))
        return x


"""## Loss

Here we define the two terms in the loss, namely the reconstruction term and the regularization term.
"""


class ReconstructionLoss(nn.Module):
    def __init__(self, loss_func: nn.Module):
        super().__init__()
        self.loss_func = loss_func

    def forward(self, p: torch.Tensor, y_hat: torch.Tensor, y: torch.Tensor):
        total_loss = p.new_tensor(0.)

        for n in range(p.shape[0]):
            loss = (p[n] * self.loss_func(y_hat[n], y)).mean()
            total_loss = total_loss + loss

        return total_loss


class RegularizationLoss(nn.Module):
    def __init__(self, lambda_p: float, max_steps: int = 1_000, device=None):
        super().__init__()

        p_g = torch.zeros((max_steps,), device=device)
        not_halted = 1.

        for k in range(max_steps):
            p_g[k] = not_halted * lambda_p
            not_halted = not_halted * (1 - lambda_p)

        self.p_g = nn.Parameter(p_g, requires_grad=False)
        self.kl_div = nn.KLDivLoss(reduction='batchmean')

    def forward(self, p: torch.Tensor):
        p = p.transpose(0, 1)
        p_g = self.p_g[None, :p.shape[1]].expand_as(p)
        return self.kl_div(p.log(), p_g)


"""## PonderNet

Finally, we have PonderNet. We use a `PyTorch Lichtning` module, which allows us to control all the aspects of training, validation and testing in the same class. Of special importance is the forward pass; for the sake of simplicity, we decided to implement a hardcoded maximum number of steps approach instead of a threshold on the cumulative probability of halting.
"""


class PonderCNN(pl.LightningModule):
    def __init__(self, n_classes, n_input, n_hidden, n_hidden_cnn, n_hidden_lin, kernel_size, max_steps, lambda_p, beta, lr):
        super().__init__()

        # attributes
        self.n_classes = n_classes
        self.max_steps = max_steps
        self.lambda_p = lambda_p
        self.beta = beta
        self.n_hidden = n_hidden
        self.lr = lr

        # modules
        self.cnn = CNN(n_input=n_input, kernel_size=kernel_size, n_hidden=n_hidden_cnn)
        self.mlp = MLP(n_input=n_hidden_cnn + n_hidden, n_hidden=n_hidden_lin, n_output=n_hidden)
        self.outpt_layer = nn.Linear(n_hidden, n_classes)
        self.lambda_layer = nn.Linear(n_hidden, 1)

        # losses
        self.loss_rec = ReconstructionLoss(nn.CrossEntropyLoss())
        self.loss_reg = RegularizationLoss(self.lambda_p, max_steps=self.max_steps, device=self.device)

        # metrics
        self.accuracy = torchmetrics.Accuracy()

        # save hparams on W&B
        self.save_hyperparameters()

    def forward(self, x):
        # extract batch size for QoL
        batch_size = x.shape[0]

        # propagate to get h_1
        h = x.new_zeros((batch_size, self.n_hidden))
        embedding = self.cnn(x)
        concat = torch.cat([embedding, h], 1)
        h = self.mlp(concat)

        # lists to save p_n, y_n
        p = []
        y = []

        # vectors to save intermediate values
        un_halted_prob = h.new_ones((batch_size,))  # unhalted probability till step n
        halted = h.new_zeros((batch_size,))  # stopping step

        # vector to save the outputs at stopping time for inference
        y_m = h.new_zeros((batch_size, self.n_classes))
        step_halted = h.new_zeros((batch_size,))

        # main loop
        for n in range(1, self.max_steps + 1):
            # obtain lambda_n
            if n == self.max_steps:
                lambda_n = h.new_ones(h.shape[0])
            else:
                lambda_n = torch.sigmoid(self.lambda_layer(h))[:, 0]

            if torch.any(torch.isnan(lambda_n)):
                print(embedding)
                print(concat)
                print(x)
                print(x.size())
                print(n)

            # obtain output and p_n
            y_n = self.outpt_layer(h)
            p_n = un_halted_prob * lambda_n

            # track unhalted probability and flip coin to halt
            un_halted_prob = un_halted_prob * (1 - lambda_n)
            halt = torch.bernoulli(lambda_n) * (1 - halted)

            # append p_n, y_n
            p.append(p_n)
            y.append(y_n)

            # update y_m
            halt_tiled = halt.view(-1, 1).repeat((1, self.n_classes))
            y_m = y_m * (1 - halt_tiled) + y_n * halt_tiled

            # update halted
            halted = halted + halt
            step_halted = step_halted + n * halt

            # propagate to obtain h_n
            embedding = self.cnn(x)
            concat = torch.cat([embedding, h], 1)
            h = self.mlp(concat)

            # break if we are in inference and all elements have halted
            if not self.training and halted.sum() == batch_size:
                break

        return torch.stack(p), torch.stack(y), step_halted, y_m

    def training_step(self, batch, batch_idx):
        _, steps, loss, acc = self._get_preds_steps_loss_acc(batch)

        # logging
        self.log('train/steps', steps)
        self.log('train/accuracy', acc)
        self.log('train/loss', loss)

        return loss

    def validation_step(self, batch, batch_idx):
        preds, steps, loss, acc = self._get_preds_steps_loss_acc(batch)

        # logging
        self.log('val/steps', steps)
        self.log('val/accuracy', acc)
        self.log('val/loss', loss)

        # for custom callback
        return preds

    def test_step(self, batch, batch_idx):
        _, steps, loss, acc = self._get_preds_steps_loss_acc(batch)

        # logging
        self.log('test/steps', steps)
        self.log('test/accuracy', acc)
        self.log('test/loss', loss)

    def configure_optimizers(self):
        optimizer = Adam(self.parameters(), lr=self.lr)
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": ReduceLROnPlateau(optimizer, mode='max', verbose=True),
                "monitor": 'val/accuracy',
                "interval": 'epoch',
                "frequency": 1
            }
        }
        return optimizer

    def configure_callbacks(self):
        early_stopping = EarlyStopping(monitor='val/accuracy', mode='max', patience=3)
        model_checkpoint = ModelCheckpoint(monitor="val/accuracy", mode='max')
        return [early_stopping, model_checkpoint]

    def _get_preds_steps_loss_acc(self, batch):
        # extract the batch
        data, target = batch

        # forward pass
        p, y_hat, halted, y_hat_sampled = self(data)

        # calculate the loss
        loss_rec_ = self.loss_rec(p, y_hat, target)
        loss_reg_ = self.loss_reg(p)
        loss = loss_rec_ + self.beta * loss_reg_

        # calculate the accuracy
        preds = torch.argmax(y_hat_sampled, dim=1)
        acc = self.accuracy(preds, target)

        # calculate the average number of steps
        steps = halted.mean()

        return preds, steps, loss, acc


"""## Custom callbacks

Nice little callback that will log images as well as their ground truth and prediction on W&B. It is optional, but specially nice when training on the rotated dataset.
"""


class LogPredictionsCallback(Callback):
    def on_validation_batch_end(
            self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):
        """Called when the validation batch ends."""

        # `outputs` comes from `LightningModule.validation_step`
        # which corresponds to our model predictions in this case

        # Let's log 20 sample image predictions from first batch
        if batch_idx == 0:
            n = 8
            x, y = batch
            # we can directly use `wandb` for logging custom objects (image, video, audio, modecules and any other custom plot)
            wandb.log({'examples': [wandb.Image(x_i, caption=f'Ground Truth: {y_i}\nPrediction: {y_pred}')
                                    for x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))]})


"""# Run interpolation

Load the MNIST dataset with no rotations and train PonderNet on it. Make sure to edit the `WandbLogger` call so that you log the experiment on your account's desired project.
"""

# initialize datamodule and model
mnist = MNIST_DataModule(batch_size=config.batch_size, num_workers=config.num_workers)
model = PonderCNN(n_classes=config.n_classes,
                  n_input=config.n_input,
                  n_hidden=config.n_hidden,
                  n_hidden_cnn=config.n_hidden_cnn,
                  n_hidden_lin=config.n_hidden_lin,
                  kernel_size=config.kernel_size,
                  max_steps=config.max_steps,
                  lambda_p=config.lambda_p,
                  beta=config.beta,
                  lr=config.lr)

# setup logger
wandb_logger_i = WandbLogger(project='PonderNet', name='interpolation', offline=False)
wandb_logger_i.watch(model)

trainer = Trainer(
    logger=wandb_logger_i,                   # W&B integration
    callbacks=[LogPredictionsCallback(),
               EarlyStopping(monitor='val/accuracy', mode='max', patience=3),
               ModelCheckpoint(monitor="val/accuracy", mode='max')],    # add callback
    gpus=-1,                                 # use all available GPU's
    gradient_clip_val=config.grad_norm_clip,  # gradient clipping
    precision=16,                            # train in half precision
    auto_lr_find=True,                       # find the best lr automatically
    deterministic=True)                      # for reproducibility

# find the best lr automatically
trainer.tune(model, datamodule=mnist)

# fit the model
trainer.fit(model, datamodule=mnist)

# evaluate on the test set
trainer.test(model, datamodule=mnist)

wandb.finish()

"""# Run extrapolation
Train PonderNet on slightly rotated MNIST pictures, while testing on more pronounced rotations. As before, make sure to edit the `WandbLogger` accordingly.
"""

# define transformations
transform_22 = transforms.Compose([
    transforms.RandomRotation(degrees=22.5),
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
transform_45 = transforms.Compose([
    transforms.RandomRotation(degrees=45),
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
transform_67 = transforms.Compose([
    transforms.RandomRotation(degrees=67.5),
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
transform_90 = transforms.Compose([
    transforms.RandomRotation(degrees=90),
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# create dataloaders for the other datasets
test_loader_45 = torch.utils.data.DataLoader(
    MNIST('./', train=False, download=True,
          transform=transform_45),
    batch_size=config.batch_size, num_workers=config.num_workers)

test_loader_67 = torch.utils.data.DataLoader(
    MNIST('./', train=False, download=True,
          transform=transform_67),
    batch_size=config.batch_size, num_workers=config.num_workers)

test_loader_90 = torch.utils.data.DataLoader(
    MNIST('./', train=False, download=True,
          transform=transform_90),
    batch_size=config.batch_size, num_workers=config.num_workers)

# initialize datamodule and model
mnist = MNIST_DataModule(batch_size=config.batch_size, num_workers=config.num_workers,
                         train_transform=transform_22, test_transform=transform_22)
model = PonderCNN(n_classes=config.n_classes,
                  n_input=config.n_input,
                  n_hidden=config.n_hidden,
                  n_hidden_cnn=config.n_hidden_cnn,
                  n_hidden_lin=config.n_hidden_lin,
                  kernel_size=config.kernel_size,
                  max_steps=config.max_steps,
                  lambda_p=config.lambda_p,
                  beta=config.beta,
                  lr=config.lr)

# setup logger
wandb_logger_e = WandbLogger(project='PonderNet', name='extrapolation', offline=True)
wandb_logger_e.watch(model)

trainer = Trainer(
    logger=wandb_logger_e,                   # W&B integration
    callbacks=[LogPredictionsCallback()],    # add callback
    gpus=-1,                                 # use all available GPU's
    gradient_clip_val=config.grad_norm_clip,  # gradient clipping
    precision=16,                            # train in half precision
    auto_lr_find=True,                       # find the best lr automatically
    deterministic=True)                      # for reproducibility

# find the best lr automatically
trainer.tune(model, datamodule=mnist)

# fit the model
trainer.fit(model, datamodule=mnist)

# evaluate on the test sets
trainer.test(model, datamodule=mnist)
trainer.test(model, test_dataloaders=test_loader_45)
trainer.test(model, test_dataloaders=test_loader_67)
trainer.test(model, test_dataloaders=test_loader_90)

# stop W&B
wandb.finish()
